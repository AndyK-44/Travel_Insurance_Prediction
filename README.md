# Travel_Insurance_Prediction
Excited to Share My Latest Machine Learning Project on Predicting Travel Insurance Uptake

As part of my journey to learn and apply machine learning, I explored the Travel Insurance dataset (available at Kaggle).

In this project, I compared four models; Logistic Regression, Random Forest, K-Neighbors, and Gradient Boosting accuracy to predict whether an individual is likely or not likely to take up travel insurance based on the following features:

    Age                         
    Government Sector Worker    
    GraduateOrNot               
    AnnualIncome                
    FamilyMembers               
    ChronicDiseases             
    FrequentFlyer               
    EverTravelledAbroad 


KEY TAKEWAYS

  The training and testing datasets were split into the ratio of 0.7 and 0.3 respectively which reflected to 477 observations for the training set and 205 observations for the testing set. The Random Forest model and Gradient Boosting achieved the highest accuracy with 81.7% and 81.4% respectively. The Logistic regression performed relatively poorly with just an accuracy of 68% while K-Neighbours classifier achieved a 78% accuracy. 
  
  While all the four models are classification algorithms, the key difference lies in their ability to handle complex non-linear relationships. The logistic regression is better suited for simpler linear relationships and while the other three machile learning models excel at capturing complex patterns within data. However, the machine learning models are harder to interpret. 
